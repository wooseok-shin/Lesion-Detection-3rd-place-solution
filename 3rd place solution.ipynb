{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 병변 검출 AI 경진대회 (Object Detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object Detection 대회를 제대로 참여한 건 처음이었는데 운 좋게 수상할 수 있었던 것 같습니다. 특히 Public에서는 6등이었는데 Private에서 3등이 될 수 있었던 가장 큰 요인은 YoloV5와 MMDetection의 모델들을 사용하여 앙상블 시 다양성을 준 것이 주요한 요인이지 않았나 싶습니다.\n",
    "\n",
    "- 학습에 사용한 모델은 아래와 같습니다.\n",
    "    - YoloV5 - Ensemble 시 다양성을 주기위해 이미지 사이즈, 모델 사이즈, Fold를 다양하게 학습하였습니다.\n",
    "    - 다만, 적절한 구조 및 하이퍼파라미터를 찾는것에 시간을 소모하여 Fold0인 모델이 많습니다.\n",
    "        - L - Fold0: size(384, 512)\n",
    "        - X - Fold0: size(320, 384, 480), Fold2(480), Fold4(640)\n",
    "\n",
    "    - MMDetection\n",
    "        - CenterNet(R-18) - 512size (Fold0, Fold1)\n",
    "        - RetinaNet(R-101) - 1024size (Fold0)\n",
    "\n",
    "    <br>\n",
    "\n",
    "    - 모든 모델은 학습 시 Pretrained Weight을 yolov5 & mmdet github로부터 받아서 사용하였습니다.\n",
    "\n",
    "- 추론\n",
    "    - 모델\n",
    "        - 7개의 YoloV5 모델 Single image inference = 7개의 결과\n",
    "        - 7개의 YoloV5 모델 Test Time Augmentation(TTA) = 7개의 결과\n",
    "        - 2개의 CenterNet Single image inference = 2개의 결과\n",
    "        - 2개의 CenterNet Test Time Augmentation(TTA) = 2개의 결과\n",
    "        - 1개의 RetinaNet Single image inference = 1개의 결과 (TTA는 꽤 오래걸려서 수행하지 않았습니다)\n",
    "    \n",
    "    - Ensemble\n",
    "        - BBox를 Ensemble하는 방법으로는 캐글에서도 많이 사용되는 Weighted Boxes Fusion (https://github.com/ZFTurbo/Weighted-Boxes-Fusion)라는 기법을 사용하였습니다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "- 최종적으로 총 10개의 모델을 학습하고 TTA를 포함하여 19개의 결과물을 Ensemble하였습니다.\n",
    "- 제 Github에서 trained Weight을 다운로드하여 결과를 추론해보실 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-1. Requirements\n",
    "- Ubuntu 18.04, Cuda 11.1\n",
    "- Anaconda - Python 3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- numpy\n",
    "- pandas\n",
    "- opencv-python\n",
    "- python-dateutil\n",
    "- pytz\n",
    "- six\n",
    "- matplotlib\n",
    "- natsort\n",
    "- tqdm\n",
    "- scikit-learn\n",
    "- ensemble-boxes\n",
    "- torch==1.9.0 torchvision 0.10.0 with cuda 11.1 (YoloV5)\n",
    "- torch==1.8.0 torchvision 0.9.0 with cuda 11.1 (MMDetection)\n",
    "- mmcv-full\n",
    "- mmdet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YoloV5의 경우 학습 속도가 torch 1.9.0 버전에서 더 빨라서 1.9.0을 사용하였고, MMDetection의 경우 1.9.0에서 코드가 돌아가지 않는 현상이 발생하여 1.8.0으로 다르게 사용하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make_dataset.sh나 install_mmdet.sh에서 필요한 라이브러리가 설치되도록 하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-2 Directory 구조\n",
    "- data/ 폴더에 train, test폴더의 json파일과 lesion.names, 그리고 etc/ 폴더의 lesion.yaml파일을 준비하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    ".\n",
    "├── README.md\n",
    "├── data\n",
    "│   ├── class_id_info.csv\n",
    "│   ├── lesion.names\n",
    "│   ├── sample_submission.csv\n",
    "│   └── train\n",
    "        ├── train_100000.json\n",
    "        ├── train_100001.json\n",
    "│   ├── test\n",
    "        ├── test_200000.json\n",
    "        ├── test_200001.json\n",
    "\n",
    "├── convert2Yolo\n",
    "│   └── requirements.txt\n",
    "│   ├── ...\n",
    "\n",
    "├── etc\n",
    "│   ├── lesion_0f.yaml\n",
    "│   ├── lesion_2f.yaml\n",
    "│   └── lesion_4f.yaml\n",
    "\n",
    "└── yolov5\n",
    "    ├── data\n",
    "    ├── train.py\n",
    "    ├── detect.py\n",
    "    ...\n",
    "\n",
    "├── mmdet_configs\n",
    "│   ├── coco_detection.py\n",
    "│   ├── coco_detection_1f.py\n",
    "│   ├── ...\n",
    "\n",
    "├── mmdetection\n",
    "│   ├── mmdet\n",
    "│   ├── tools\n",
    "│   └── work_dirs\n",
    "│   ├── ...\n",
    "\n",
    "├── pretrain_weights\n",
    "│   ├── centernet_resnet18_dcnv2_140e_coco_20210702_155131-c8cd631f.pth\n",
    "│   └── retinanet_r101_fpn_2x_coco_20200131-5560aee8.pth\n",
    "\n",
    "├── trained_weights\n",
    "│   ├── exp1.pt\n",
    "│   ├── exp2.pt\n",
    "│   ├── ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 아래 코드들은 모두 Ubuntu terminal에서 실행하여야 합니다.\n",
    "#### 자세한 코드는 제 Github에서 확인해주시면 감사하겠습니다. (https://github.com/wooseok-shin/Lesion-Detection-3rd-place-solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Make dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. 필요한 라이브러리와 본 대회의 데이터 Fromat에서 COCO 데이터 Format으로 변형하여 줍니다. (이여름님의 코드 공유를 참고하였습니다. 감사합니다! https://dacon.io/competitions/official/235855/codeshare/3729?page=1&dtype=recent)\n",
    "\n",
    "- 2. 이후 COCO Format은 MMdetection에 사용하고, YoloV5 Format을 맞춰주기 위해 https://github.com/ssaru/convert2Yolo에서 배포하는 코드를 Clone하여 사용하였습니다.\n",
    "\n",
    "- 3. YoloV5 Format 데이터를 train/valid로 split해줍니다.\n",
    "\n",
    "- 4. YoloV5의 Repository를 clone하고 YAML 파일을 만들어줍니다. (제 GitHub etc폴더에 올려져 있습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sh make_dataset.sh\n",
    "pip install Pillow cycler kiwisolver numpy pandas opencv-python python-dateutil pytz six matplotlib natsort tqdm scikit-learn ensemble-boxes\n",
    "pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# 1. Make coco format (for mmdetection & yolov5)\n",
    "python convert_coco_format.py\n",
    "\n",
    "# 2. Coco to Yolov5 format dataset\n",
    "git clone https://github.com/ssaru/convert2Yolo.git\n",
    "cd convert2Yolo\n",
    "mkdir ../data/yolo_labels/\n",
    "## lesion.names 파일을 ../data/ 위치에 만들어주어야 함. (현재는 만들어져있음.)\n",
    "python example.py --datasets COCO --img_path ../data/train_imgs/ --label ../data/annos/train_annotations_full.json --convert_output_path ../data/yolo_labels/ --img_type \".png\" --manifest_path ./ --cls_list_file ../data/lesion.names\n",
    "cd ..\n",
    "\n",
    "# 3. Yolov5 train/valid split\n",
    "python split_yolo_dataset.py\n",
    "\n",
    "# 4. Yolov5 clone & Prerequisite\n",
    "git clone https://github.com/ultralytics/yolov5.git\n",
    "cd yolov5\n",
    "pip install -r requirements.txt\n",
    "cp ../etc/* ./data/    # Yolov5 학습을 위해 yolov5/data/경로에 yaml 파일을 만들어 주어야함 (학습에 필요한 경로 설정) - 미리 첨부해놓은 파일 복사해서 해당 폴더에 넣어주기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Train YoloV5 models (Yolov5 L,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sh train_yolov5.sh\n",
    "# Train seven models\n",
    "\n",
    "## Ensemble 시 다양성을 많이 주기위해 이미지 사이즈, 모델 사이즈, Fold를 다양하게 학습\n",
    "cd yolov5\n",
    "\n",
    "### 2 x yolov5l (0Fold: 384,512 size)\n",
    "python train.py --project weights/ --name=exp1 --img 384 --batch 16 --epochs 125 --data lesion_0f.yaml --weights yolov5l.pt --save-period 5 --workers 4\n",
    "python train.py --project weights/ --name=exp2 --img 512 --batch 16 --epochs 200 --data lesion_0f.yaml --weights yolov5l.pt --save-period 5 --workers 4\n",
    "\n",
    "### 5 x yolov5x (0Fold: 320,384,480 size & 2,4Fold)\n",
    "python train.py --project weights/ --name=exp3 --img 320 --batch 16 --epochs 125 --data lesion_0f.yaml --weights yolov5x.pt --save-period 5 --workers 4\n",
    "python train.py --project weights/ --name=exp4 --img 384 --batch 16 --epochs 125 --data lesion_0f.yaml --weights yolov5x.pt --save-period 5 --workers 4\n",
    "python train.py --project weights/ --name=exp5 --img 480 --batch 16 --epochs 125 --data lesion_0f.yaml --weights yolov5x.pt --save-period 5 --workers 4\n",
    "python train.py --project weights/ --name=exp6 --img 480 --batch 16 --epochs 125 --data lesion_2f.yaml --weights yolov5x.pt --save-period 5 --workers 4\n",
    "python train.py --project weights/ --name=exp7 --img 640 --batch 16 --epochs 125 --data lesion_4f.yaml --weights yolov5x.pt --save-period 5 --workers 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Train MMDetection models (CenterNet, RetinaNet)\n",
    "- 기존에 Yolov5를 학습시킬 때 torch 1.9.0 버전을 사용하였는데, MMDetection에서 1.9.0이 돌아가지 않는 현상이 있었습니다.\n",
    "- 따라서, torch를 1.8.0, torchvision을 0.10.0으로 재설치 해주었습니다. (가상환경을 따로할 시 상관없음)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sh mmdet_install.sh (mmcv 및 mmdet 설치)\n",
    "\n",
    "# Install mmcv and mmdet\n",
    "pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.8.0/index.html\n",
    "pip install mmdet\n",
    "\n",
    "# MMDetection을 처음부터 다운로드 받아서 할 시 Git Clone 수행\n",
    "git clone https://github.com/open-mmlab/mmdetection.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 처음엔 Yolov5 계열로만 Ensemble을 하다가 성능 향상이 더뎌진 것 같아 전혀 다른 구조의 모델을 사용하였습니다.\n",
    "- 그 중에서 CenterNet(R-18)과 RetinaNet(R-101)을 학습하였습니다.\n",
    "- Backbone으로 CBNetV2나 Detector로 HTC 등 COCO 기준으로 더 좋은 성능을 보이는 모델도 학습해보았으나 오버피팅 문제가 발생하여 가벼운 구조를 채택하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sh train_mmdet.sh\n",
    "# Train three models\n",
    "cp -r mmdet_configs/ mmdetection/configs/    # 수정해놓은 config 폴더 복사해서 mmdetection/configs 폴더에 넣기\n",
    "\n",
    "# Download pretrain weight from mmdetection github (CenterNet R-18 and Retina R-101)\n",
    "wget https://download.openmmlab.com/mmdetection/v2.0/centernet/centernet_resnet18_dcnv2_140e_coco/centernet_resnet18_dcnv2_140e_coco_20210702_155131-c8cd631f.pth -P ./pretrain_weights/\n",
    "wget https://download.openmmlab.com/mmdetection/v2.0/retinanet/retinanet_r101_fpn_2x_coco/retinanet_r101_fpn_2x_coco_20200131-5560aee8.pth -P ./pretrain_weights/\n",
    "\n",
    "cd mmdetection\n",
    "\n",
    "## 1.Centernet Fold 0\n",
    "bash tools/dist_train.sh configs/mmdet_configs/exp8_centernet_resnet18_dcnv2_140e_coco.py 1\n",
    "\n",
    "## 2.Centernet Fold 1\n",
    "bash tools/dist_train.sh configs/mmdet_configs/exp9_centernet_resnet18_dcnv2_140e_coco.py 1\n",
    "\n",
    "## 3.Retina ResNet-101 Fold 0\n",
    "bash tools/dist_train.sh configs/mmdet_configs/exp10_retinanet_r101_fpn_1x_coco.py 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) YoloV5 models (7 single inference + 7 TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sh inference_yolo.sh\n",
    "cd yolov5\n",
    "\n",
    "# yolov5/results/expN 폴더에 txt파일 예측 결과물 저장\n",
    "\n",
    "## Single image inference 7 models\n",
    "python detect.py --weights weights/exp1/weights/epoch120.pt --project ../results/ --name exp1_single --img 384 --source ../data/test_imgs --save-txt --save-conf --conf-thres 0.001 --iou-thres 0.6 --nosave\n",
    "python detect.py --weights weights/exp2/weights/epoch150.pt --project ../results/ --name exp2_single --img 512 --source ../data/test_imgs --save-txt --save-conf --conf-thres 0.001 --iou-thres 0.6 --nosave\n",
    "python detect.py --weights weights/exp3/weights/epoch120.pt --project ../results/ --name exp3_single --img 320 --source ../data/test_imgs --save-txt --save-conf --conf-thres 0.001 --iou-thres 0.6 --nosave\n",
    "python detect.py --weights weights/exp4/weights/epoch120.pt --project ../results/ --name exp4_single --img 384 --source ../data/test_imgs --save-txt --save-conf --conf-thres 0.001 --iou-thres 0.6 --nosave\n",
    "python detect.py --weights weights/exp5/weights/epoch120.pt --project ../results/ --name exp5_single --img 480 --source ../data/test_imgs --save-txt --save-conf --conf-thres 0.001 --iou-thres 0.6 --nosave\n",
    "python detect.py --weights weights/exp6/weights/epoch120.pt --project ../results/ --name exp6_single --img 480 --source ../data/test_imgs --save-txt --save-conf --conf-thres 0.001 --iou-thres 0.6 --nosave\n",
    "python detect.py --weights weights/exp7/weights/epoch120.pt --project ../results/ --name exp7_single --img 640 --source ../data/test_imgs --save-txt --save-conf --conf-thres 0.001 --iou-thres 0.6 --nosave\n",
    "\n",
    "## TTA 7 models\n",
    "python detect.py --weights weights/exp1/weights/epoch120.pt --project ../results/ --name exp1_tta --img 384 --source ../data/test_imgs --save-txt --save-conf --conf-thres 0.001 --iou-thres 0.6 --nosave --augment\n",
    "python detect.py --weights weights/exp2/weights/epoch150.pt --project ../results/ --name exp2_tta --img 512 --source ../data/test_imgs --save-txt --save-conf --conf-thres 0.001 --iou-thres 0.6 --nosave --augment\n",
    "python detect.py --weights weights/exp3/weights/epoch120.pt --project ../results/ --name exp3_tta --img 320 --source ../data/test_imgs --save-txt --save-conf --conf-thres 0.001 --iou-thres 0.6 --nosave --augment\n",
    "python detect.py --weights weights/exp4/weights/epoch120.pt --project ../results/ --name exp4_tta --img 384 --source ../data/test_imgs --save-txt --save-conf --conf-thres 0.001 --iou-thres 0.6 --nosave --augment\n",
    "python detect.py --weights weights/exp5/weights/epoch120.pt --project ../results/ --name exp5_tta --img 480 --source ../data/test_imgs --save-txt --save-conf --conf-thres 0.001 --iou-thres 0.6 --nosave --augment\n",
    "python detect.py --weights weights/exp6/weights/epoch120.pt --project ../results/ --name exp6_tta --img 480 --source ../data/test_imgs --save-txt --save-conf --conf-thres 0.001 --iou-thres 0.6 --nosave --augment\n",
    "python detect.py --weights weights/exp7/weights/epoch120.pt --project ../results/ --name exp7_tta --img 640 --source ../data/test_imgs --save-txt --save-conf --conf-thres 0.001 --iou-thres 0.6 --nosave --augment\n",
    "\n",
    "cd ../\n",
    "\n",
    "# Yolov5 infernece format to submission format\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp1_single --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp2_single --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp3_single --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp4_single --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp5_single --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp6_single --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp7_single --test_img_path data/test_imgs/\n",
    "\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp1_tta --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp2_tta --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp3_tta --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp4_tta --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp5_tta --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp6_tta --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp7_tta --test_img_path data/test_imgs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) MMDetection models (3 single + 2 TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sh inference_mmdet.sh\n",
    "\n",
    "# Single image inference 3 models (각각 25epoch, 28epoch, 22epoch에서 Best valid score가 나옴)\n",
    "python inference_mmdet.py --exp_name exp8_centernet_resnet18_dcnv2_140e_coco --checkpoint epoch_25\n",
    "python inference_mmdet.py --exp_name exp9_centernet_resnet18_dcnv2_140e_coco --checkpoint epoch_28\n",
    "python inference_mmdet.py --exp_name exp10_retinanet_r101_fpn_1x_coco --checkpoint epoch_22\n",
    "\n",
    "# TTA 2 models\n",
    "python inference_mmdet.py --use_tta _tta --exp_name exp8_centernet_resnet18_dcnv2_140e_coco --checkpoint epoch_25\n",
    "python inference_mmdet.py --use_tta _tta --exp_name exp9_centernet_resnet18_dcnv2_140e_coco --checkpoint epoch_28\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. WBF Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WBF Ensemble - 7 Yolo models x 2(NoTTA, TTA) + 2 CenterNet x 2(NoTTA, TTA) + 1 RetinaNet\n",
    "- Make final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python ensemble_wbf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Public, Private Score 복원: 학습된 모델 Weight를 불러와서 inference하기 (Git으로부터 Weights Download)\n",
    "- 10개의 Weight를 불러와야하는데 Github 서버 문제인지 가끔 하나씩 안불러오는 경우가 있는 것 같습니다.\n",
    "- exp1~exp10까지 다 불러와졌는지 확인이 필요합니다.\n",
    "- 한 두개가 빠져있으면 해당 폴더를 삭제하고 다시 실행하거나 하나씩 wget하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download trained weight from my github (https://github.com/wooseok-shin/Lesion-Detection-3rd-place-solution)\n",
    "wget -i https://raw.githubusercontent.com/wooseok-shin/Lesion-Detection-3rd-place-solution/main/load_trained_weight.txt -P trained_weights\n",
    "\n",
    "\n",
    "# Yolov5 7x2 models\n",
    "cd yolov5\n",
    "# yolov5/results/expN 폴더에 txt파일 예측 결과물 저장\n",
    "\n",
    "## Single image inference 7 models\n",
    "python detect.py --weights ../trained_weights/exp1.pt --project ../results/ --name exp1_single --img 384 --source ../data/test_imgs --save-txt --save-conf --nosave --conf-thres 0.001 --iou-thres 0.6\n",
    "python detect.py --weights ../trained_weights/exp2.pt --project ../results/ --name exp2_single --img 512 --source ../data/test_imgs --save-txt --save-conf --nosave --conf-thres 0.001 --iou-thres 0.6\n",
    "python detect.py --weights ../trained_weights/exp3.pt --project ../results/ --name exp3_single --img 320 --source ../data/test_imgs --save-txt --save-conf --nosave --conf-thres 0.001 --iou-thres 0.6\n",
    "python detect.py --weights ../trained_weights/exp4.pt --project ../results/ --name exp4_single --img 384 --source ../data/test_imgs --save-txt --save-conf --nosave --conf-thres 0.001 --iou-thres 0.6\n",
    "python detect.py --weights ../trained_weights/exp5.pt --project ../results/ --name exp5_single --img 480 --source ../data/test_imgs --save-txt --save-conf --nosave --conf-thres 0.001 --iou-thres 0.6\n",
    "python detect.py --weights ../trained_weights/exp6.pt --project ../results/ --name exp6_single --img 480 --source ../data/test_imgs --save-txt --save-conf --nosave --conf-thres 0.001 --iou-thres 0.6\n",
    "python detect.py --weights ../trained_weights/exp7.pt --project ../results/ --name exp7_single --img 640 --source ../data/test_imgs --save-txt --save-conf --nosave --conf-thres 0.001 --iou-thres 0.6\n",
    "\n",
    "## TTA 7 models\n",
    "python detect.py --weights ../trained_weights/exp1.pt --project ../results/ --name exp1_tta --img 384 --source ../data/test_imgs --save-txt --save-conf --nosave --conf-thres 0.001 --iou-thres 0.6 --augment\n",
    "python detect.py --weights ../trained_weights/exp2.pt --project ../results/ --name exp2_tta --img 512 --source ../data/test_imgs --save-txt --save-conf --nosave --conf-thres 0.001 --iou-thres 0.6 --augment\n",
    "python detect.py --weights ../trained_weights/exp3.pt --project ../results/ --name exp3_tta --img 320 --source ../data/test_imgs --save-txt --save-conf --nosave --conf-thres 0.001 --iou-thres 0.6 --augment\n",
    "python detect.py --weights ../trained_weights/exp4.pt --project ../results/ --name exp4_tta --img 384 --source ../data/test_imgs --save-txt --save-conf --nosave --conf-thres 0.001 --iou-thres 0.6 --augment\n",
    "python detect.py --weights ../trained_weights/exp5.pt --project ../results/ --name exp5_tta --img 480 --source ../data/test_imgs --save-txt --save-conf --nosave --conf-thres 0.001 --iou-thres 0.6 --augment\n",
    "python detect.py --weights ../trained_weights/exp6.pt --project ../results/ --name exp6_tta --img 480 --source ../data/test_imgs --save-txt --save-conf --nosave --conf-thres 0.001 --iou-thres 0.6 --augment\n",
    "python detect.py --weights ../trained_weights/exp7.pt --project ../results/ --name exp7_tta --img 640 --source ../data/test_imgs --save-txt --save-conf --nosave --conf-thres 0.001 --iou-thres 0.6 --augment\n",
    "\n",
    "cd ../\n",
    "\n",
    "## Yolov5 infernece format to submission format\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp1_single --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp2_single --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp3_single --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp4_single --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp5_single --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp6_single --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp7_single --test_img_path data/test_imgs/\n",
    "\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp1_tta --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp2_tta --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp3_tta --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp4_tta --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp5_tta --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp6_tta --test_img_path data/test_imgs/\n",
    "python inference_yolo_agg.py --dir_prefix results/ --exp_name exp7_tta --test_img_path data/test_imgs/\n",
    "\n",
    "\n",
    "# MMdetection 5 models\n",
    "cp -r mmdet_configs/ mmdetection/configs/ # Configs 복사 및 덮어쓰기\n",
    "\n",
    "## Single image inference 3 models\n",
    "python inference_load_mmdet.py --exp_name exp8_centernet_resnet18_dcnv2_140e_coco --checkpoint exp8\n",
    "python inference_load_mmdet.py --exp_name exp9_centernet_resnet18_dcnv2_140e_coco --checkpoint exp9\n",
    "python inference_load_mmdet.py --exp_name exp10_retinanet_r101_fpn_1x_coco --checkpoint exp10\n",
    "\n",
    "## TTA 2 models\n",
    "python inference_load_mmdet.py --use_tta _tta --exp_name exp8_centernet_resnet18_dcnv2_140e_coco --checkpoint exp8\n",
    "python inference_load_mmdet.py --use_tta _tta --exp_name exp9_centernet_resnet18_dcnv2_140e_coco --checkpoint exp9\n",
    "\n",
    "# Final WBF Ensemble\n",
    "python ensemble_wbf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01f05b19fdc99af4f55aaab63d9ae217971d29dee087a9ccc0ec464f62fede62"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('plant': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
